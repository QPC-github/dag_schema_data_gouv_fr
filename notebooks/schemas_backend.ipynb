{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import yaml\n",
    "from git import Repo, Git\n",
    "import os\n",
    "import shutil\n",
    "from table_schema_to_markdown import convert_source\n",
    "import frictionless\n",
    "import glob\n",
    "import json\n",
    "import jsonschema\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import codecs\n",
    "import requests\n",
    "from urllib import parse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "CACHE_FOLDER = TMP_FOLDER+'cache'\n",
    "DATA_FOLDER1 = TMP_FOLDER+'data'\n",
    "DATA_FOLDER2 = TMP_FOLDER+'data2'\n",
    "ERRORS_REPORT = []\n",
    "SCHEMA_INFOS = {}\n",
    "SCHEMA_CATALOG = {}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyze validity of every release of every schema"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Loading yaml file containing all schemas that we want to display in schema.data.gouv.fr\n",
    "r = requests.get(LIST_SCHEMAS_YAML)\n",
    "\n",
    "with open(TMP_FOLDER+'repertoires.yml', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "with open(TMP_FOLDER+'repertoires.yml', \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def clean_and_create_folder(folder):\n",
    "    \"\"\"Remove local folder if exist and (re)create it\"\"\"\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder)\n",
    "    os.mkdir(folder)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_consolidated_version(tag):\n",
    "    \"\"\"Analyze tag from a code source release, cast it to acceptable semver version X.X.X\"\"\"\n",
    "    valid_version = True\n",
    "    # Removing 'v' or 'V' from tag\n",
    "    version_items = str(tag).replace('v','').replace('V','').split('.')\n",
    "    # Add a patch number if only 2 items\n",
    "    if len(version_items) == 2: version_items.append('0')\n",
    "    # If more than 3, do not accept tag\n",
    "    if len(version_items) > 3: valid_version = False\n",
    "    # Verify if all items are digits\n",
    "    for v in version_items:\n",
    "        if not v.isdigit():\n",
    "            valid_version = False\n",
    "    # Return semver version and validity of it\n",
    "    return '.'.join(version_items), valid_version"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def manage_errors(repertoire_slug, version, reason):\n",
    "    \"\"\"Create dictionnary that will populate ERRORS_REPORT object\"\"\"\n",
    "    errors = {}\n",
    "    errors['schema'] = repertoire_slug\n",
    "    errors['version'] = version\n",
    "    errors['type'] = reason\n",
    "    ERRORS_REPORT.append(errors)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def check_schema(repertoire_slug, conf, schema_type):\n",
    "    \"\"\"Check validity of schema and all of its releases\"\"\"\n",
    "    # schema_name in schema.data.gouv.fr is referenced by group and repo name in Git. Ex : etalab / schema-irve\n",
    "    schema_name = '/'.join(conf['url'].split('.git')[0].split('/')[-2:])\n",
    "    # define source folder and create it\n",
    "    # source folder will help us to checkout to every release and analyze source code for each one\n",
    "    src_folder = CACHE_FOLDER + '/' + schema_name + '/'\n",
    "    os.makedirs(src_folder)\n",
    "    # clone repo in source folder\n",
    "    Repo.clone_from(conf['url'], src_folder)\n",
    "    repo = Repo(src_folder)\n",
    "    # get tags of repo\n",
    "    tags = sorted(repo.tags, key=lambda t: t.commit.committed_datetime)\n",
    "    \n",
    "    list_schemas = {}\n",
    "    \n",
    "    # Defining SCHEMA_INFOS object for website use\n",
    "    SCHEMA_INFOS[schema_name] = {}\n",
    "    SCHEMA_INFOS[schema_name]['homepage'] = conf['url']\n",
    "    SCHEMA_INFOS[schema_name]['external_doc'] = conf['external_doc'] if 'external_doc' in conf else None\n",
    "    SCHEMA_INFOS[schema_name]['external_tool'] = conf['external_tool'] if 'external_tool' in conf else None\n",
    "    SCHEMA_INFOS[schema_name]['type'] = conf['type']\n",
    "    SCHEMA_INFOS[schema_name]['email'] = conf['email']\n",
    "    SCHEMA_INFOS[schema_name]['labels'] = conf['labels'] if 'labels' in conf else None\n",
    "    SCHEMA_INFOS[schema_name]['consolidation_dataset_id'] = conf['consolidation'] if 'consolidation' in conf else None\n",
    "    SCHEMA_INFOS[schema_name]['versions'] = {}\n",
    "    \n",
    "    # for every tags\n",
    "    for t in tags:\n",
    "        # get semver version and validity of it\n",
    "        version, valid_version = get_consolidated_version(t)\n",
    "        # if semver ok\n",
    "        if(valid_version):\n",
    "            # define destination folder and create it\n",
    "            # destination folder will store pertinents files for website for each version of each schema\n",
    "            dest_folder = DATA_FOLDER1 + '/' + schema_name + '/' + version + '/'\n",
    "            os.makedirs(dest_folder)\n",
    "            # checkout to current version\n",
    "            g = Git(src_folder)\n",
    "            g.checkout(str(t))\n",
    "            \n",
    "            conf_schema = None\n",
    "            # Managing validation differently for each type of schema\n",
    "            # tableschema will use frictionless package\n",
    "            # jsonschema will use jsonschema package\n",
    "            # other will only check if schema.yml file is present and contain correct information\n",
    "            if(schema_type == 'tableschema'):\n",
    "                list_schemas = manage_tableschema(src_folder, dest_folder, list_schemas, version, schema_name)\n",
    "            if(schema_type == 'jsonschema'):\n",
    "                list_schemas, conf_schema = manage_jsonschema(src_folder, dest_folder, list_schemas, version, schema_name)\n",
    "            if(schema_type == 'other'):\n",
    "                list_schemas, conf_schema = manage_other(src_folder, dest_folder, list_schemas, version, schema_name)\n",
    "\n",
    "          \n",
    "    # Find latest valid version and create a specific folder 'latest' copying files in it (for website use)\n",
    "    latest_folder, list_schemas, schema_file = manage_latest_folder(conf, list_schemas, schema_name)\n",
    "    # Complete catalog with all relevant information of schema in it\n",
    "    schema_to_add_to_catalog = generate_catalog_object(latest_folder, list_schemas, schema_file, schema_type, schema_name, conf_schema)\n",
    "    return schema_to_add_to_catalog"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def check_datapackage(repertoire_slug, conf, schema_type):\n",
    "    \"\"\"Check validity of schemas from a datapackage repo for all of its releases\"\"\"\n",
    "    # schema_name in schema.data.gouv.fr is referenced by group and repo name in Git. Ex : etalab / schema-irve\n",
    "    \n",
    "    # define source folder and create it\n",
    "    # source folder will help us to checkout to every release and analyze source code for each one\n",
    "    src_folder = CACHE_FOLDER + '/' + '/'.join(conf['url'].split('.git')[0].split('/')[-2:]) + '/'\n",
    "    os.makedirs(src_folder)\n",
    "    # clone repo in source folder\n",
    "    Repo.clone_from(conf['url'], src_folder)\n",
    "    repo = Repo(src_folder)\n",
    "    # get tags of repo\n",
    "    tags = sorted(repo.tags, key=lambda t: t.commit.committed_datetime)\n",
    "    \n",
    "    \n",
    "    list_schemas = {}\n",
    "    schemas_to_add_to_catalog = []\n",
    "    # Verify that a file datapackage.json is present\n",
    "    if(os.path.isfile(src_folder + 'datapackage.json')):\n",
    "        # Validate it with frictionless package\n",
    "        frictionless_report = frictionless.validate(src_folder + 'datapackage.json')\n",
    "        # If datapackage release is valid, then\n",
    "        if(frictionless_report['valid'] == True):\n",
    "            with open(src_folder + 'datapackage.json') as out:\n",
    "                dp = json.load(out)\n",
    "            \n",
    "            \n",
    "            README = '\\n# ' + dp['title'] + '\\n\\nCe Data Package contient plusieurs schémas. Merci de sélectionner le schémas que vous souhaitez consulter.'\n",
    "            \n",
    "            schemas_dp = [r['schema'] for r in dp['resources'] if 'schema' in r]\n",
    "            print(schemas_dp)\n",
    "            \n",
    "            for schema in schemas_dp:\n",
    "                with open(src_folder + schema) as out:\n",
    "                    schema_json = json.load(out)\n",
    "                    \n",
    "                schema_name = '/'.join(conf['url'].split('.git')[0].split('/')[-2:]) + '/' + schema_json['name']\n",
    "                README = README + '\\n - [' + schema_json['title'] + '](/' + schema_name + ')'\n",
    "                # Defining SCHEMA_INFOS object for website use\n",
    "                SCHEMA_INFOS[schema_name] = {}\n",
    "                SCHEMA_INFOS[schema_name]['homepage'] = conf['url']\n",
    "                SCHEMA_INFOS[schema_name]['external_doc'] = conf['external_doc'] if 'external_doc' in conf else None\n",
    "                SCHEMA_INFOS[schema_name]['external_tool'] = conf['external_tool'] if 'external_tool' in conf else None\n",
    "                SCHEMA_INFOS[schema_name]['type'] = 'tableschema'\n",
    "                SCHEMA_INFOS[schema_name]['email'] = conf['email']\n",
    "                SCHEMA_INFOS[schema_name]['versions'] = {}\n",
    "                SCHEMA_INFOS[schema_name]['datapackage'] = dp['title']\n",
    "                SCHEMA_INFOS[schema_name]['datapackage_id'] = dp['name']\n",
    "                SCHEMA_INFOS[schema_name]['labels'] = conf['labels'] if 'labels' in conf else None\n",
    "                SCHEMA_INFOS[schema_name]['consolidation_dataset_id'] = conf['consolidation'] if 'consolidation' in conf else None\n",
    "\n",
    "                # for every tags\n",
    "                for t in tags:\n",
    "                    # get semver version and validity of it\n",
    "                    version, valid_version = get_consolidated_version(t)\n",
    "                    # if semver ok\n",
    "                    if(valid_version):\n",
    "                        # define destination folder and create it\n",
    "                        # destination folder will store pertinents files for website for each version of each schema\n",
    "                        dest_folder = DATA_FOLDER1 + '/' + schema_name + '/' + version + '/'\n",
    "                        print(dest_folder)\n",
    "                        os.makedirs(dest_folder)\n",
    "                        # checkout to current version\n",
    "                        g = Git(src_folder)\n",
    "                        g.checkout(str(t))\n",
    "\n",
    "                        conf_schema = None\n",
    "                        # Managing validation \n",
    "                        list_schemas = manage_tableschema(src_folder, dest_folder, list_schemas, version, schema_name, schema)\n",
    "\n",
    "                # Find latest valid version and create a specific folder 'latest' copying files in it (for website use)\n",
    "                latest_folder, list_schemas, schema_file = manage_latest_folder(conf, list_schemas, schema_name)\n",
    "                \n",
    "                # Complete catalog with all relevant information of schema in it\n",
    "                schema_to_add_to_catalog = generate_catalog_object(latest_folder, list_schemas, schema_file, 'tableschema', schema_name, conf_schema, dp)\n",
    "                \n",
    "                schemas_to_add_to_catalog.append(schema_to_add_to_catalog)\n",
    "        else:\n",
    "            print('not valid')\n",
    "    else:\n",
    "        print('no datapackage')\n",
    "    print(schemas_to_add_to_catalog)\n",
    "\n",
    "    return schemas_to_add_to_catalog, README+'\\n\\n'\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def manage_tableschema(src_folder, dest_folder, list_schemas, version, schema_name, schema_file='schema.json'):\n",
    "    \"\"\"Check validity of a schema release from tableschema type\"\"\"\n",
    "    # Verify that a file schema.json is present\n",
    "    if(os.path.isfile(src_folder + schema_file)):\n",
    "        # Validate it with frictionless package\n",
    "        frictionless_report = frictionless.validate_schema(src_folder + schema_file)\n",
    "        # If schema release is valid, then        \n",
    "        if(frictionless_report['valid'] == True):\n",
    "            list_schemas[version] = schema_file\n",
    "            # We complete info of version\n",
    "            SCHEMA_INFOS[schema_name]['versions'][version] = {}\n",
    "            SCHEMA_INFOS[schema_name]['versions'][version]['pages'] = []\n",
    "            # We check for list of normalized files if it is present in source code, if so, we copy paste them into dest folder\n",
    "            for f in [schema_file, 'README.md', 'SEE_ALSO.md', 'CHANGELOG.md', 'CONTEXT.md']:\n",
    "                if(os.path.isfile(src_folder + f)):\n",
    "                    shutil.copyfile(src_folder + f,dest_folder + f)\n",
    "                    # if it is a markdown file, we will read them as page in website\n",
    "                    if(f[-3:] == '.md'):\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append(f)\n",
    "                    # if it is the schema, we indicate it as it in object\n",
    "                    if(f == schema_file):\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['schema_url'] = '/' + schema_name + '/' + version + '/' + schema_file\n",
    "            # Create documentation file and save it\n",
    "            with open(dest_folder + 'documentation.md', \"w\") as out:\n",
    "                try:\n",
    "                    # From schema.json, we use tableschema_to_markdown package to convert it in a\n",
    "                    # readable mardown file that will be use for documentation\n",
    "                    convert_source(dest_folder + schema_file, out, 'page',[])\n",
    "                    SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append('documentation.md')\n",
    "                except:\n",
    "                    # if conversion is on error, we add it to ERRORS_REPORT\n",
    "                    manage_errors(repertoire_slug, version, 'convert to markdown')\n",
    "        # If schema release is not valid, we remove it from DATA_FOLDER1\n",
    "        else:\n",
    "            manage_errors(repertoire_slug, version, 'tableschema validation')\n",
    "            shutil.rmtree(dest_folder)\n",
    "    # If there is no schema.json, schema release is not valid, we remove it from DATA_FOLDER1\n",
    "    else:\n",
    "        manage_errors(repertoire_slug, version, 'missing ' + schema_file)\n",
    "        shutil.rmtree(dest_folder)\n",
    "    \n",
    "    return list_schemas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def manage_jsonschema(src_folder, dest_folder, list_schemas, version, schema_name):\n",
    "    \"\"\"Check validity of a schema release from jsonschema type\"\"\"\n",
    "    conf_schema = None\n",
    "    # Verify that a file schemas.yml is present\n",
    "    # This file will indicate title, description of jsonschema (it is a prerequisite asked by schema.data.gouv.fr)\n",
    "    # This file will also indicate which file store the jsonschema schema\n",
    "    if(os.path.isfile(src_folder + 'schemas.yml')):\n",
    "        try:\n",
    "            with open(src_folder + 'schemas.yml', \"r\") as f:\n",
    "                conf_schema = yaml.safe_load(f)\n",
    "                if('schemas' in conf_schema):\n",
    "                    s = conf_schema['schemas'][0]\n",
    "                    # Verify if jsonschema file indicate in schemas.yml is present, then load it\n",
    "                    if(os.path.isfile(src_folder + s['path'])):\n",
    "                        with open(src_folder + s['path'], \"r\") as f:\n",
    "                            schema_data = json.load(f)\n",
    "                        # Validate schema with jsonschema package\n",
    "                        jsonschema.validators.validator_for(schema_data).check_schema(schema_data)\n",
    "                        list_schemas[version] = s['path']\n",
    "                        # We complete info of version\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version] = {}\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['pages'] = []\n",
    "                        # We check for list of normalized files if it is present in source code, if so, we copy paste them into dest folder\n",
    "                        for f in ['README.md', 'SEE_ALSO.md', 'CHANGELOG.md', 'CONTEXT.md', s['path']]:\n",
    "                            if(os.path.isfile(src_folder + f)):\n",
    "                                shutil.copyfile(src_folder + f,dest_folder + f)\n",
    "                            # if it is a markdown file, we will read them as page in website\n",
    "                            if(f[-3:] == '.md'):\n",
    "                                SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append(f)\n",
    "                            # if it is the schema, we indicate it as it in object\n",
    "                            if(f == s['path']):\n",
    "                                SCHEMA_INFOS[schema_name]['versions'][version]['schema_url'] = '/' + schema_name + '/' + version + '/' + s['path']\n",
    "        # If schema release is not valid, we remove it from DATA_FOLDER1\n",
    "        except:\n",
    "            manage_errors(repertoire_slug, version, 'jsonschema validation')\n",
    "            shutil.rmtree(dest_folder)\n",
    "    # If there is no schemas.yml, schema release is not valid, we remove it from DATA_FOLDER1    \n",
    "    else:\n",
    "        manage_errors(repertoire_slug, version, 'missing schemas.yml')\n",
    "        shutil.rmtree(dest_folder)\n",
    "    \n",
    "    return list_schemas, conf_schema\n",
    "     "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def manage_other(src_folder, dest_folder, list_schemas, version, schema_name):\n",
    "    \"\"\"Check validity of a schema release from other type\"\"\"\n",
    "    conf_schema = None\n",
    "    # Verify that a file schema.yml is present\n",
    "    # This file will indicate title, description of schema (it is a prerequisite asked by schema.data.gouv.fr)\n",
    "    if(os.path.isfile(src_folder + 'schema.yml')):\n",
    "        try:\n",
    "            with open(src_folder + 'schema.yml', \"r\") as f:\n",
    "                conf_schema = yaml.safe_load(f)\n",
    "            list_schemas[version] = 'schema.yml'\n",
    "            # We complete info of version\n",
    "            SCHEMA_INFOS[schema_name]['versions'][version] = {}\n",
    "            SCHEMA_INFOS[schema_name]['versions'][version]['pages'] = []\n",
    "            # We check for list of normalized files if it is present in source code, if so, we copy paste them into dest folder\n",
    "            for f in ['README.md', 'SEE_ALSO.md', 'CHANGELOG.md', 'CONTEXT.md', 'schema.yml']:\n",
    "                if(os.path.isfile(src_folder + f)):\n",
    "                    shutil.copyfile(src_folder + f,dest_folder + f)\n",
    "                    # if it is a markdown file, we will read them as page in website\n",
    "                    if(f[-3:] == '.md'):\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append(f)\n",
    "                    # if it is the schema, we indicate it as it in object\n",
    "                    if(f == 'schema.yml'):\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['schema_url'] = '/' + schema_name + '/' + version + '/' + 'schema.yml'\n",
    "        # If schema release is not valid, we remove it from DATA_FOLDER1\n",
    "        except:\n",
    "            manage_errors(repertoire_slug, version, 'validation of type other')\n",
    "            shutil.rmtree(dest_folder)\n",
    "    # If there is no schema.yml, schema release is not valid, we remove it from DATA_FOLDER1    \n",
    "    else:\n",
    "        manage_errors(repertoire_slug, version, 'missing schema.yml')\n",
    "        shutil.rmtree(dest_folder)\n",
    "    \n",
    "    return list_schemas, conf_schema\n",
    "     "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def manage_latest_folder(conf, list_schemas, schema_name):\n",
    "    \"\"\"Create latest folder containing all files from latest valid version of a schema\"\"\"\n",
    "    # Get all valid version from a schema by analyzing folders, then sort them to get latest valid version and related folder\n",
    "    subfolders = [ f.name for f in os.scandir(DATA_FOLDER1 + '/' + schema_name + '/') if f.is_dir() ]\n",
    "    subfolders.sort()\n",
    "    latest_version_folder = DATA_FOLDER1 + '/' + schema_name + '/' + subfolders[-1] + '/'\n",
    "    # Determine latest folder path then mkdir it\n",
    "    latest_folder = DATA_FOLDER1 + '/' + schema_name + '/latest/'\n",
    "    os.makedirs(latest_folder)\n",
    "    # For every file in latest valid version folder, copy them into \n",
    "    for f in glob.glob(latest_version_folder + '*'):\n",
    "        shutil.copyfile(f,latest_folder + f.split('/')[-1])\n",
    "    # For website need, copy paste latest README into root of schema folder\n",
    "    shutil.copyfile(latest_version_folder+'README.md','/'.join(latest_version_folder.split('/')[:-2])+'/README.md')\n",
    "    # Indicate in schema_info object name of latest schema\n",
    "    SCHEMA_INFOS[schema_name]['latest'] = subfolders[-1]\n",
    "    return latest_folder, list_schemas, list_schemas[subfolders[-1]]\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def generate_catalog_object(latest_folder, list_schemas, schema_file, schema_type, schema_name, obj_info=None, datapackage=None):\n",
    "    \"\"\"Generate dictionnary containing all relevant information for catalog\"\"\"\n",
    "    # If tableschema, relevant information are directly into schema.json, \n",
    "    # if not, relevant info are in yaml files with are stored in obj_info variable\n",
    "    if(schema_type == 'tableschema'):\n",
    "        with open(latest_folder + schema_file, \"r\") as f:\n",
    "            schema = json.load(f)\n",
    "    else:\n",
    "        schema = obj_info\n",
    "    # Complete dictionnary with relevant info needed in catalog\n",
    "    mydict = {}\n",
    "    mydict['name'] = schema_name\n",
    "    mydict['title'] = schema['title']\n",
    "    mydict['description'] = schema['description']\n",
    "    mydict['schema_url'] = 'https://schema.data.gouv.fr/schemas/' + schema_name + '/latest/' + schema_file\n",
    "    mydict['schema_type'] = schema_type\n",
    "    mydict['contact'] = conf['email']\n",
    "    mydict['examples'] = schema['resources'] if 'resources' in schema else []\n",
    "    mydict['labels'] = conf['labels'] if 'labels' in conf else []\n",
    "    mydict['consolidation_dataset_id'] = conf['consolidation'] if 'consolidation' in conf else None\n",
    "    mydict['versions'] = []\n",
    "    for sf in list_schemas:\n",
    "        mydict2 = {}\n",
    "        mydict2['version_name'] = sf\n",
    "        mydict2['schema_url'] = 'https://schema.data.gouv.fr/schemas/' + schema_name + '/' + sf + '/' + list_schemas[sf]\n",
    "        mydict['versions'].append(mydict2)\n",
    "    # These four following property are not in catalog spec \n",
    "    mydict['external_doc'] = conf['external_doc'] if 'external_doc' in conf else None\n",
    "    mydict['external_tool'] = conf['external_tool'] if 'external_tool' in conf else None\n",
    "    mydict['homepage'] = conf['url']\n",
    "    if(datapackage):\n",
    "        mydict['datapackage_title'] = datapackage['title']\n",
    "        mydict['datapackage_name'] = '/'.join(schema_name.split('/')[:-1])\n",
    "        mydict['datapackage_description'] = datapackage['description'] if 'description' in datapackage else None\n",
    "    return mydict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Clean and (re)create CACHE AND DATA FOLDER\n",
    "clean_and_create_folder(CACHE_FOLDER)\n",
    "clean_and_create_folder(DATA_FOLDER1)\n",
    "\n",
    "# Initiate Catalog\n",
    "SCHEMA_CATALOG['$schema'] = 'https://opendataschema.frama.io/catalog/schema-catalog.json'\n",
    "SCHEMA_CATALOG['version'] = 1\n",
    "SCHEMA_CATALOG['schemas'] = []\n",
    "\n",
    "# For every schema in repertoires.yml, check it\n",
    "for repertoire_slug, conf in config.items():\n",
    "    try:\n",
    "        if(conf['type'] != 'datapackage'):\n",
    "            schema_to_add_to_catalog = check_schema(repertoire_slug, conf, conf['type'])\n",
    "            SCHEMA_CATALOG['schemas'].append(schema_to_add_to_catalog)\n",
    "        else:\n",
    "            schemas_to_add_to_catalog, README = check_datapackage(repertoire_slug, conf, conf['type'])\n",
    "            for schema in schemas_to_add_to_catalog:\n",
    "                SCHEMA_CATALOG['schemas'].append(schema)\n",
    "            with open(DATA_FOLDER1 + '/' + '/'.join(conf['url'].split('.git')[0].split('/')[-2:]) + '/README.md', 'w') as out:\n",
    "                out.write(README)\n",
    "        # Append info to SCHEMA_CATALOG\n",
    "        print('--- {} processed'.format(repertoire_slug))\n",
    "    except:\n",
    "        print('--- {} failed to process'.format(repertoire_slug)) \n",
    "\n",
    "\n",
    "schemas_scdl = SCHEMA_CATALOG.copy()\n",
    "schemas_transport = SCHEMA_CATALOG.copy()\n",
    "\n",
    "# Save catalog to schemas.json file\n",
    "with open(DATA_FOLDER1 + '/schemas.json', 'w') as fp:\n",
    "    json.dump(SCHEMA_CATALOG, fp)\n",
    "\n",
    "schemas_scdl['schemas'] = [x for x in schemas_scdl['schemas'] if 'Socle Commun des Données Locales' in x['labels']]\n",
    "\n",
    "schemas_transport['schemas'] = [x for x in schemas_transport['schemas'] if 'transport.data.gouv.fr' in x['labels']]\n",
    "\n",
    "with open(DATA_FOLDER1 + '/schemas-scdl.json', 'w') as fp:\n",
    "    json.dump(schemas_scdl, fp)\n",
    "\n",
    "with open(DATA_FOLDER1 + '/schemas-transport-data-gouv-fr.json', 'w') as fp:\n",
    "    json.dump(schemas_transport, fp)\n",
    "\n",
    "# Save schemas_infos to schema-infos.json file\n",
    "with open(DATA_FOLDER1 + '/schema-infos.json', 'w') as fp:\n",
    "    json.dump(SCHEMA_INFOS, fp)\n",
    "    \n",
    "# Save errors to errors.json file\n",
    "with open(DATA_FOLDER1 + '/errors.json', 'w') as fp:\n",
    "    json.dump(ERRORS_REPORT, fp)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- irve processed\n",
      "['schema_lieux.json', 'schema_services.json', 'schema_acteurs.json']\n",
      "./data/geoffreyaldebert/poc-multi-tableschema/lieux/0.1.0/\n",
      "./data/geoffreyaldebert/poc-multi-tableschema/lieux/0.2.0/\n",
      "./data/geoffreyaldebert/poc-multi-tableschema/services/0.1.0/\n",
      "./data/geoffreyaldebert/poc-multi-tableschema/services/0.2.0/\n",
      "./data/geoffreyaldebert/poc-multi-tableschema/acteurs/0.1.0/\n",
      "./data/geoffreyaldebert/poc-multi-tableschema/acteurs/0.2.0/\n",
      "[{'name': 'geoffreyaldebert/poc-multi-tableschema/lieux', 'title': 'Lieux', 'description': 'Schéma de lieux', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/poc-multi-tableschema/lieux/latest/schema_lieux.json', 'schema_type': 'tableschema', 'contact': 'contact@transport.beta.gouv.fr', 'examples': [], 'versions': [{'version_name': '0.1.0', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/poc-multi-tableschema/lieux/0.1.0/schema_lieux.json'}, {'version_name': '0.2.0', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/poc-multi-tableschema/lieux/0.2.0/schema_lieux.json'}], 'external_doc': None, 'external_tool': None, 'homepage': 'https://github.com/geoffreyaldebert/poc-multi-tableschema', 'datapackage_title': 'Lieux proposant des services', 'datapackage_name': 'geoffreyaldebert/poc-multi-tableschema', 'datapackage_description': \"data package d'exemple pour les lieux proposant des services\"}, {'name': 'geoffreyaldebert/poc-multi-tableschema/services', 'title': 'Services', 'description': 'Schéma de services', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/poc-multi-tableschema/services/latest/schema_services.json', 'schema_type': 'tableschema', 'contact': 'contact@transport.beta.gouv.fr', 'examples': [], 'versions': [{'version_name': '0.1.0', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/poc-multi-tableschema/services/0.1.0/schema_services.json'}, {'version_name': '0.2.0', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/poc-multi-tableschema/services/0.2.0/schema_services.json'}], 'external_doc': None, 'external_tool': None, 'homepage': 'https://github.com/geoffreyaldebert/poc-multi-tableschema', 'datapackage_title': 'Lieux proposant des services', 'datapackage_name': 'geoffreyaldebert/poc-multi-tableschema', 'datapackage_description': \"data package d'exemple pour les lieux proposant des services\"}, {'name': 'geoffreyaldebert/poc-multi-tableschema/acteurs', 'title': 'Acteurs', 'description': 'Schéma des acteurs', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/poc-multi-tableschema/acteurs/latest/schema_acteurs.json', 'schema_type': 'tableschema', 'contact': 'contact@transport.beta.gouv.fr', 'examples': [], 'versions': [{'version_name': '0.1.0', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/poc-multi-tableschema/acteurs/0.1.0/schema_acteurs.json'}, {'version_name': '0.2.0', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/poc-multi-tableschema/acteurs/0.2.0/schema_acteurs.json'}], 'external_doc': None, 'external_tool': None, 'homepage': 'https://github.com/geoffreyaldebert/poc-multi-tableschema', 'datapackage_title': 'Lieux proposant des services', 'datapackage_name': 'geoffreyaldebert/poc-multi-tableschema', 'datapackage_description': \"data package d'exemple pour les lieux proposant des services\"}]\n",
      "--- lieux-representant-services processed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparation for website"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## schema relative files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def find_md_links(md):\n",
    "    \"\"\"Returns dict of links in markdown:\n",
    "    'regular': [foo](some.url)\n",
    "    'footnotes': [foo][3]\n",
    "    \n",
    "    [3]: some.url\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/a/30738268/2755116\n",
    "    INLINE_LINK_RE = re.compile(r'\\[([^\\]]+)\\]\\(([^)]+)\\)')\n",
    "    FOOTNOTE_LINK_TEXT_RE = re.compile(r'\\[([^\\]]+)\\]\\[(\\d+)\\]')\n",
    "    FOOTNOTE_LINK_URL_RE = re.compile(r'\\[(\\d+)\\]:\\s+(\\S+)')\n",
    "\n",
    "    links = list(INLINE_LINK_RE.findall(md))\n",
    "    footnote_links = dict(FOOTNOTE_LINK_TEXT_RE.findall(md))\n",
    "    footnote_urls = dict(FOOTNOTE_LINK_URL_RE.findall(md))\n",
    "\n",
    "    footnotes_linking = []\n",
    "        \n",
    "    for key in footnote_links.keys():\n",
    "        footnotes_linking.append((footnote_links[key], footnote_urls[footnote_links[key]]))\n",
    "\n",
    "    return links"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def cleanLinksDocumentation(dest_folder):\n",
    "    \"\"\"Custom cleaning for links in markdown\"\"\"\n",
    "    # For every documentation.md file, do some custom cleaning for links\n",
    "    file = codecs.open(dest_folder + 'documentation.md', \"r\", \"utf-8\")\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    # Find all links in file\n",
    "    links = find_md_links(data)\n",
    "    # For each one, lower string then manage space ; _ ; --- and replace them by -\n",
    "    for (name, link) in links:\n",
    "        if(link.startswith('#')):\n",
    "            newlink = link.lower()\n",
    "            newlink = newlink.replace(' ','-')\n",
    "            newlink = newlink.replace('_','-')\n",
    "            newlink = unidecode(newlink, \"utf-8\")\n",
    "            newlink = newlink.replace('---','-')\n",
    "            data = data.replace(link,newlink)\n",
    "    # Save modifications\n",
    "    with open(dest_folder + 'documentation.md', 'w', encoding='utf-8') as fin:\n",
    "        fin.write(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def addFrontToMarkdown(dest_folder, f):\n",
    "    \"\"\"Custom add to every markdown files\"\"\"\n",
    "    # for every markdown files\n",
    "    file = codecs.open(dest_folder + f, \"r\", \"utf-8\")\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    # Add specific tag for website interpretation\n",
    "    data = \"<MenuSchema />\\n\\n\"+data\n",
    "    # Exception scdl Budget not well interpreted by vuepress\n",
    "    data = data.replace('<DocumentBudgetaire>', 'DocumentBudgetaire')\n",
    "    # Save modification\n",
    "    with open(dest_folder + f, 'w', encoding='utf-8') as fin:\n",
    "        fin.write(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def getListOfFiles(dirName):\n",
    "    \"\"\"Get list off all files in a specific folder\"\"\"\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "    return allFiles\n",
    "                "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Get list of all files in DATA_FOLDER\n",
    "files = []\n",
    "files = getListOfFiles(DATA_FOLDER1)\n",
    "# Create list of file that we do not want to copy paste\n",
    "avoid_files = [DATA_FOLDER1 + '/' + s['name'] + '/README.md' for s in SCHEMA_CATALOG['schemas']]\n",
    "# for every file\n",
    "for f in files:\n",
    "    # if it is a markdown, add custom front to content\n",
    "    if(f[-3:] == '.md'):\n",
    "        addFrontToMarkdown('/'.join(f.split('/')[:-1])+'/', f.split('/')[-1])\n",
    "    # if it is the documentation file, clean links on it\n",
    "    if(f.split('/')[-1] == 'documentation.md'):\n",
    "        cleanLinksDocumentation('/'.join(f.split('/')[:-1])+'/')\n",
    "    # if it is a README file (except if on avoid_list), copy paste it to root folder of schema (for website use)\n",
    "    # That will create README file with name X.X.X.md (X.X.X corresponding to a specific version)\n",
    "    if(f.split('/')[-1] == 'README.md'):\n",
    "        if f not in avoid_files:\n",
    "            shutil.copyfile(f, f.replace('/README.md','.md'))\n",
    "\n",
    "# Clean and (re)create DATA_FOLDER2, then copy paste all DATA_FOLDER1 into DATA_FOLDER2\n",
    "# DATA_FOLDER1 will be use to contain all markdown files\n",
    "# DATA_FOLDER2 will be use to contain all yaml and json files \n",
    "# This is needed for vuepress that need to store page in one place and 'resources' in another\n",
    "if os.path.exists(DATA_FOLDER2):\n",
    "    shutil.rmtree(DATA_FOLDER2)\n",
    "shutil.copytree(DATA_FOLDER1, DATA_FOLDER2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'./data2'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## stats file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def get_contributors(url):\n",
    "    \"\"\"Get list off all contributors of a specific git repo\"\"\"\n",
    "    parse_url = parse.urlsplit(url)\n",
    "    # if github, use github api\n",
    "    if('github.com' in parse_url.netloc):\n",
    "        api_url =  parse_url.scheme+'://api.github.com/repos/'+parse_url.path[1:].replace('.git','')+'/contributors'\n",
    "    # else, use gitlab api\n",
    "    else:\n",
    "        api_url =  parse_url.scheme+'://'+parse_url.netloc+'/api/v4/projects/'+parse_url.path[1:].replace('/','%2F').replace('.git','')+'/repository/contributors'\n",
    "    try:\n",
    "        r = requests.get(api_url)\n",
    "        return len(r.json())\n",
    "    except:\n",
    "        return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# For every issue, request them by label schema status (en investigation or en construction)\n",
    "mydict = {}\n",
    "labels = ['construction', 'investigation']\n",
    "# For each label, get relevant info via github api of schema.data.gouv.fr repo\n",
    "try:\n",
    "    for l in labels:\n",
    "        r = requests.get('https://api.github.com/repos/etalab/schema.data.gouv.fr/issues?q=is%3Aopen+is%3Aissue&labels=Sch%C3%A9ma%20en%20'+l)\n",
    "        mydict[l] = []\n",
    "        for issue in r.json():\n",
    "            mydict2 = {}\n",
    "            mydict2['created_at'] = issue['created_at']\n",
    "            mydict2['labels'] = [l]\n",
    "            mydict2['nb_comments'] = issue['comments']\n",
    "            mydict2['title'] = issue['title']\n",
    "            mydict2['url'] = issue['html_url']\n",
    "            mydict[l].append(mydict2)\n",
    "\n",
    "    # Find number of current issue in schema.data.gouv.fr repo\n",
    "    r = requests.get('https://api.github.com/repos/etalab/schema.data.gouv.fr/issues?q=is%3Aopen+is%3Aissue')\n",
    "    mydict['nb_issues'] = len(r.json())\n",
    "\n",
    "    # for every schema, find relevant info in data.gouv.fr API\n",
    "    mydict['references'] = {}\n",
    "    for s in SCHEMA_CATALOG['schemas']:\n",
    "        r = requests.get('https://www.data.gouv.fr/api/1/datasets/?schema='+s['name'])\n",
    "        mydict['references'][s['name']] = {}\n",
    "        mydict['references'][s['name']]['dgv_resources'] = r.json()['total']\n",
    "        mydict['references'][s['name']]['title'] = s['title']\n",
    "        mydict['references'][s['name']]['contributors'] = get_contributors(s['homepage'])\n",
    "\n",
    "    # Save stats infos to stats.json file\n",
    "    with open(DATA_FOLDER2 + '/stats.json', 'w') as fp:\n",
    "        json.dump(mydict, fp)\n",
    "except:\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare prod folders for website"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def remove_all_files_extension(folder, extension):\n",
    "    \"\"\"Remove all file of a specific extension in a folder\"\"\"\n",
    "    files = []\n",
    "    files = getListOfFiles(folder)\n",
    "    for f in files:\n",
    "        if f[-1*len(extension):] == extension:\n",
    "            os.remove(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Remove all markdown from DATA_FOLDER1 and all json, yaml and yml file of DATA_FOLDER2\n",
    "remove_all_files_extension(DATA_FOLDER2, '.md')\n",
    "remove_all_files_extension(DATA_FOLDER1, '.json')\n",
    "remove_all_files_extension(DATA_FOLDER1, '.yml')\n",
    "remove_all_files_extension(DATA_FOLDER1, '.yaml')\n",
    "\n",
    "'''\n",
    "# Remove actual currend prod folder in website\n",
    "folder_to_remove = glob.glob(TMP_FOLDER+'schema.data.gouv.fr/site/site/*/')\n",
    "folder_to_remove.append(TMP_FOLDER+'schema.data.gouv.fr/site/site/.vuepress/public/schemas')\n",
    "for folder in folder_to_remove:\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../site/site/.vuepress/public/stats.json'"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "'''\n",
    "# Copy all markdown in site folder, all resources in site/.vuepress/public/schemas/folder\n",
    "shutil.copytree(DATA_FOLDER1, 'schema.data.gouv.fr/site/site/')\n",
    "shutil.copytree(DATA_FOLDER2, 'schema.data.gouv.fr/site/site/.vuepress/public/schemas/')\n",
    "\n",
    "# Copy three main json files into another public folder\n",
    "shutil.copyfile('schema.data.gouv.fr/site/site/.vuepress/public/schemas/schemas.json','schema.data.gouv.fr/site/site/.vuepress/public/schemas.json')\n",
    "shutil.copyfile('schema.data.gouv.fr/site/site/.vuepress/public/schemas/schema-infos.json','schema.data.gouv.fr/site/site/.vuepress/public/schema-infos.json')\n",
    "shutil.copyfile('schema.data.gouv.fr/site/site/.vuepress/public/schemas/errors.json','schema.data.gouv.fr/site/site/.vuepress/public/errors.json')\n",
    "shutil.copyfile('schema.data.gouv.fr/site/site/.vuepress/public/schemas/stats.json','schema.data.gouv.fr/site/site/.vuepress/public/stats.json')\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6224c542726492982829fea3fc5066cee43ae47aa086ecd8a28c7a969dcd0d45"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}